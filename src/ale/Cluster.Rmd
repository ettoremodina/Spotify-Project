---
title: "Cluster"
author: "Alessandra Pescina"
date: "2024-01-21"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
seed=2024
```



```{r}
numeric_data<-readRDS("../../data/data_numeric_cleaned2.RData")
numeric_red<-numeric_data[,-c(5,11,12,13)]

```


## Outlier detection (non usare)

```{r}

data_MCD<-numeric_red[,-5]
set.seed(seed)
library(robustbase)
fit_MCD <- covMcd(x = data_MCD, 
                  alpha = .75,
                  nsamp = "best") 
p<-11
ind_best <-
  which(
    mahalanobis(
      x = data_MCD,
      center = fit_MCD$center,
      cov = fit_MCD$cov
    ) <= qchisq(p = .975, df = p)
  )
ind_out<-setdiff(1:nrow(numeric_red),ind_best)

cleaned<-data_MCD[-ind_out, ]
numeric_red_cleaned<-numeric_red[-ind_out,]
```


## Cluster of songs 

euclidean, 9 cov, ward.D2
```{r}
library(StatMatch)
n<-dim(numeric_red)[1]
p<-dim(numeric_red)[2]
distance <- 'canberra' # manhattan, canberra
linkages <- c('single', 'average', 'complete', 'ward.D2')

# distance matrix:
data.dist_e <- dist(numeric_red, method=distance)
#data.dist<-mahalanobis.dist(numeric_red)
#temp<-dist(data.dist)
# plot:
#image(1:n,1:n, as.matrix(data.dist), main=paste('metrics: ', distance), asp=1, xlab='', ylab='')


# perform hierarchical clustering:
#data.s <- hclust(data.dist, method=linkages[1])
#data.a <- hclust(data.dist, method=linkages[2])
#data.c <- hclust(data.dist, method=linkages[3])
#data.w <- hclust(data.dist, method=linkages[4])

# perform hierarchical clustering maha:
data.s <- hclust(data.dist_e, method=linkages[1])
data.a <- hclust(data.dist_e, method=linkages[2])
data.c <- hclust(data.dist_e, method=linkages[3])
data.w <- hclust(data.dist_e, method=linkages[4])

# plot dendograms:
quartz()
par(mfrow=c(2,2))

plot(data.s, main=paste(distance, ' - ', linkages[1]), hang=-0.1, xlab='', labels=F, cex=0.6, sub='')
plot(data.a, main=paste(distance, ' - ', linkages[2]), hang=-0.1, xlab='', labels=F, cex=0.6, sub='')
plot(data.c, main=paste(distance, ' - ', linkages[3]), hang=-0.1, xlab='', labels=F, cex=0.6, sub='')
plot(data.w, main=paste(distance, ' - ', linkages[4]), hang=-0.1, xlab='', labels=F, cex=0.6, sub='')
```

```{r}
#cut dendogram:
k=2
data.hc<-data.w
clusters <- cutree(data.hc, k=k)


#Dimension of the Clusters
table(clusters)
```


```{r}
numeric_red$cluster<-clusters
```

## Assign people to clusters

```{r}
library(dplyr)

# Assuming your DataFrame is named 'songs_df'
# It should have columns like 'playlist_id', 'song_id', and 'cluster'
numeric_red$id<-numeric_data$id

# Count the distribution of songs in each cluster for each playlist
playlist_cluster_counts <- numeric_red %>%
  group_by(id, cluster) %>%
  summarise(count = n())

# Display the result
print(playlist_cluster_counts)

library(tidyr)
cluster_df <- playlist_cluster_counts %>%
  pivot_wider(names_from = cluster, values_from = count, values_fill = 0) %>%
  select(id, everything())
```

subtract to each cluster column expected values of songs by cluster in each playlist

```{r}
cluster_df_centered<-apply(cluster_df[,-1],2,function(x) x-mean(x))
cluster_df_centered<-as.data.frame(cluster_df_centered)
```

```{r}
#cluster_df_centered<-cluster_df[,-1]
cluster_df_centered$final<-apply(cluster_df_centered, 1, function(row) which.max(row))
table(cluster_df_centered$final)
```

## Classify people 

```{r}
survey_data<-readRDS("../../data/survey_reduced.RData")
survey_data$cluster<-cluster_df_centered$final
survey_data<-survey_data[,-c(8,13,15)]
survey_data$cluster<-as.factor(survey_data$cluster)
survey_data$cluster <- make.names(survey_data$cluster)
survey_data$campo.studi<-ifelse(is.na(survey_data$campo.studi), "others", survey_data$campo.studi)
```

decision tree-> not performing
```{r}
library(rpart)
fit<-rpart(as.factor(cluster) ~. , data=survey_data)
summary(fit)
residuals(fit)

quartz()
plot(fit)
text(fit, use.n=T)

predictions <- predict(fit, survey_data, type = "class")
conf_matrix <- confusionMatrix(predictions, as.factor(survey_data$cluster))
print(conf_matrix)
```
random forest
```{r}
library(randomForest)
library(caret)
set.seed(seed)

param_grid <- expand.grid(
  mtry = seq(4, ncol(survey_data) - 1, by = 1)  # Adjust the range as needed
)

# Define the control parameters for the cross-validation
ctrl <- trainControl(
  method = "LOOCV",  # Use leave-one-out cross-validation
  search = "grid",
  summaryFunction = defaultSummary,
  classProbs = TRUE,
  verboseIter = TRUE
)

# Fit the Random Forest model with cross-validation
rf_model <- train(
  cluster ~ .,
  data = survey_data,
  method = "rf",  # Use Random Forest
  trControl = ctrl,
  tuneGrid = param_grid
)

# Print the best model's parameters
print(rf_model$bestTune)

# Access the best Random Forest model
best_rf_model <- rf_model$finalModel
```
```{r}

#indices <- sample(1:nrow(survey_data), 60)
#train_data <- survey_data[indices, ]
#test_data <- survey_data[-indices, ]
fit_RF<-randomForest(as.factor(cluster) ~ ., data = survey_data, mtry=10, ntree=5000, replace=T)

#predictions <- predict(fit_RF, test_data, type = "class")
#conf_matrix <- confusionMatrix(predictions, as.factor(test_data$cluster))
#print(conf_matrix)
```


TODO
scegliere nuove variabili per fittare i cluster
come?
-selezione (solo variabili signfificative per differenze al 0.01)
-pca e fittare i cluster su nuove variabili

