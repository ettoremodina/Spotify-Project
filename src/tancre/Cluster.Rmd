---
title: "Cluster"
author: "Alessandra Pescina"
date: "2024-01-21"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
seed=2024
```



```{r}
numeric_data<-readRDS("../../data/data_numeric_cleaned2.RData")
numeric_red<-numeric_data[,-c(5,11,12,13)]
#pca_data <- numeric_red
#scale in (0,1) year and duration
min_max_normalize <- function(x) {
  return((x - min(x)) / (max(x) - min(x)))
}

pca_data <- numeric_data[,-13]
pca_data$duration<-  min_max_normalize(numeric_data$duration)
pca_data$year<-  min_max_normalize(numeric_data$year)
```


## PCA

```{r}
n <- dim(pca_data)[1]
p <- dim(pca_data)[2]

# checking variability: if there are some variables with a very larger variance, 
# they could driven the analysis of Principal Components:
boxplot(pca_data, las = 2, col = 'gold')

# performing PCA:
pc.data <- princomp(pca_data, scores=T)
summary(pc.data)
load.data <- pc.data$loadings

cumsum(pc.data$sd^2)/sum(pc.data$sd^2)

varmax <- max(var(pca_data[,1:dim(pca_data)[2]]))
varmax_pc <- max(pc.data$sd)
layout(matrix(c(2,3,1,3),2,byrow=T))
plot(pc.data, las=2, main='Principal components', ylim=c(0,varmax_pc^2))
barplot(sapply(pca_data,sd)^2, las=2, main='Original Variables', ylim=c(0,varmax),
        ylab='Variances')
plot(cumsum(pc.data$sd^2)/sum(pc.data$sd^2), type='b', axes=F, 
     xlab='number of components', ylab='contribution to the total variance', ylim=c(0,1))
abline(h=1, col='blue')
abline(h=0.8, lty=2, col='blue')
box()
axis(2,at=0:10/10,labels=0:10/10)
axis(1,at=1:ncol(pca_data),labels=1:ncol(pca_data),las=2)

#considero le prime 6 pca (82% della varianza spiegata, non troppo soddisfaciente ma ci provo)
```
#k numero di pc che scelgo
```{r}
k <- 6
par(mfrow = c(3,2))
for(i in 1:k) barplot(load.data[,i], ylim = c(-1, 1), main = paste('PC', i))
pca_red_data <- data.frame(pc.data$scores[,1:k])
```


## Cluster of songs 

euclidean, 6 pc, ward.D2
```{r}
library(StatMatch)
n<-dim(pca_red_data)[1]
p<-dim(pca_red_data)[2]
distance <- 'canberra' # manhattan, canberra
linkages <- c('single', 'average', 'complete', 'ward.D2')

# distance matrix:
data.dist_e <- dist(pca_red_data, method=distance)
#data.dist<-mahalanobis.dist(numeric_red)
#temp<-dist(data.dist)
# plot:
#image(1:n,1:n, as.matrix(data.dist), main=paste('metrics: ', distance), asp=1, xlab='', ylab='')


# perform hierarchical clustering:
#data.s <- hclust(data.dist, method=linkages[1])
#data.a <- hclust(data.dist, method=linkages[2])
#data.c <- hclust(data.dist, method=linkages[3])
#data.w <- hclust(data.dist, method=linkages[4])

# perform hierarchical clustering maha:
data.s <- hclust(data.dist_e, method=linkages[1])
data.a <- hclust(data.dist_e, method=linkages[2])
data.c <- hclust(data.dist_e, method=linkages[3])
data.w <- hclust(data.dist_e, method=linkages[4])

# plot dendograms:
x11()   #quartz() if form mac
par(mfrow=c(2,2))

plot(data.s, main=paste(distance, ' - ', linkages[1]), hang=-0.1, xlab='', labels=F, cex=0.6, sub='')
plot(data.a, main=paste(distance, ' - ', linkages[2]), hang=-0.1, xlab='', labels=F, cex=0.6, sub='')
plot(data.c, main=paste(distance, ' - ', linkages[3]), hang=-0.1, xlab='', labels=F, cex=0.6, sub='')
plot(data.w, main=paste(distance, ' - ', linkages[4]), hang=-0.1, xlab='', labels=F, cex=0.6, sub='')
```

```{r}
#cut dendogram:
k=2
data.hc<-data.w
clusters <- cutree(data.hc, k=k)


#Dimension of the Clusters
table(clusters)
```


```{r}
numeric_red$cluster<-clusters
```

## Assign people to clusters

```{r}
library(dplyr)

# Assuming your DataFrame is named 'songs_df'
# It should have columns like 'playlist_id', 'song_id', and 'cluster'
numeric_red$id<-numeric_data$id

# Count the distribution of songs in each cluster for each playlist
playlist_cluster_counts <- numeric_red %>%
  group_by(id, cluster) %>%
  summarise(count = n())

# Display the result
print(playlist_cluster_counts)

library(tidyr)
cluster_df <- playlist_cluster_counts %>%
  pivot_wider(names_from = cluster, values_from = count, values_fill = 0) %>%
  select(id, everything())
```

subtract to each cluster column expected values of songs by cluster in each playlist

```{r}
cluster_df_centered<-apply(cluster_df[,-1],2,function(x) x-mean(x))
cluster_df_centered<-as.data.frame(cluster_df_centered)
```

```{r}
#cluster_df_centered<-cluster_df[,-1]
cluster_df_centered$final<-apply(cluster_df_centered, 1, function(row) which.max(row))
table(cluster_df_centered$final)
```

## Classify people 

```{r}
survey_data<-readRDS("../../data/survey_reduced.RData")
survey_data$cluster<-cluster_df_centered$final
survey_data<-survey_data[,-c(8,13,15)]
survey_data$cluster<-as.factor(survey_data$cluster)
survey_data$cluster <- make.names(survey_data$cluster)
survey_data$campo.studi<-ifelse(is.na(survey_data$campo.studi), "others", survey_data$campo.studi)
```

decision tree-> not the worst
```{r}
library(rpart)
library(caret)
fit<-rpart(as.factor(cluster) ~. , data=survey_data)
summary(fit)
residuals(fit)

x11() #quartz()
plot(fit)

text(fit, use.n=T)

predictions <- predict(fit, survey_data, type = "class")
conf_matrix <- confusionMatrix(predictions, as.factor(survey_data$cluster))
print(conf_matrix)
```
random forest
```{r}
library(randomForest)
library(caret)
set.seed(2024)

param_grid <- expand.grid(
  mtry = seq(4, ncol(survey_data) - 1, by = 1)  # Adjust the range as needed
)

# Define the control parameters for the cross-validation
ctrl <- trainControl(
  method = "LOOCV",  # Use leave-one-out cross-validation
  search = "grid",
  summaryFunction = defaultSummary,
  classProbs = TRUE,
  verboseIter = TRUE
)

# Fit the Random Forest model with cross-validation
rf_model <- train(
  cluster ~ .,
  data = survey_data,
  method = "rf",  # Use Random Forest
  trControl = ctrl,
  tuneGrid = param_grid
)

# Print the best model's parameters
print(rf_model$bestTune)

# Access the best Random Forest model
best_rf_model <- rf_model$finalModel
```
```{r}

#indices <- sample(1:nrow(survey_data), 60)
#train_data <- survey_data[indices, ]
#test_data <- survey_data[-indices, ]
fit_RF<-randomForest(as.factor(cluster) ~ ., data = survey_data, mtry=10, ntree=5000, replace=T)
fit_RF
#predictions <- predict(fit_RF, test_data, type = "class")
#conf_matrix <- confusionMatrix(predictions, as.factor(test_data$cluster))
#print(conf_matrix)
```


TODO
scegliere nuove variabili per fittare i cluster
come?
-selezione (solo variabili signfificative per differenze al 0.01)
-pca e fittare i cluster su nuove variabili

